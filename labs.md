# Applied AI Engineering for the Enterprise
## Day 2 - Models and Retrieval Augmented Generation (RAG)
## Session labs 
## Revision 3.3 - 02/03/26

**Follow the startup instructions in the README.md file IF NOT ALREADY DONE!**

**NOTE: To copy and paste in the codespace, you may need to use keyboard commands - CTRL-C and CTRL-V. Chrome may work best for this.**

**Lab 1 - Working with Neural Networks**

**Purpose: In this lab, we’ll learn more about neural networks by seeing how one is coded and trained.**

1. In our repository, we have a set of Python programs to help us illustrate and work with concepts in the labs. The first set are in the *llm* subdirectory. Go to the *TERMINAL* tab in the bottom part of your codespace and change into that directory.

```
cd llm
```
<br><br>

2. For this lab, we have a simple neural net coded in Python. The file name is nn.py. Open the file either by clicking on [**llm/nn.py**](./llm/nn.py) or by entering the command below in the codespace's terminal.

```
code nn.py
```
<br><br>


3. Scroll down to around line 55. Notice the *training_inputs* data and the *training_outputs* data. Each row of the *training_outputs* is what we want the model to predict for the corresponding input row. As coded, the output for the sample inputs ends up being the same as the first element of the array.  For inputs [0,0,1] we are trying to train the model to predict [0]. For the inputs [1,0,1], we are trying to train the model to predict [1], etc. The table below may help to explain.

| **Dataset** | **Values** | **Desired Prediction** |
| :---------: | :--------: | :--------------------: |
| **1** |  0  0  1  |            0           |
| **2** |  1  1  1  |            1           |
| **3** |  1  0  1  |            1           |
| **4** |  0  1  1  |            0           |

![Code in simple nn](./images/aia-1-3.png?raw=true "Code in simple nn")

<br><br>

4. When we run the program, it will train the neural net to try and predict the outputs corresponding to the inputs. You will see the random training weights to start and then the adjusted weights to make the model predict the output. You will then be prompted to put in your own training data. We'll look at that in the next step. For now, go ahead and run the program (command below) but don't put in any inputs yet. Just notice how the weights have been adjusted after the training process.

```
python nn.py
```
![Starting run of simple nn](./images/ae4.png?raw=true "Starting run of simple nn") 

<br><br>

5. What you should see is that the weights after training are now set in a way that makes it more likely that the result will match the expected output value. (The higher positive value for the first weight means that the model has looked at the training data and realized it should "weigh" the first input higher in its prediction.) To prove this out, you can enter your own input set - just use 1's and 0's for each input. 

![Inputs to simple nn](./images/ae3.png?raw=true "Inputs to simple nn") 

<br><br>

6. After you put in your inputs, the neural net will process your input and because of the training, it should predict a result that is close to the first input value you entered (the one for *Input one*).

![Prediction close to first input](./images/ae5.png?raw=true "Prediction close to first input") 

<br><br>

7. Now, let's see what happens if we change the expected outputs to be different. In the editor for the *nn.py* file, find the line for the *training_outputs*. Modify the values in the array to be ([[0],[1],[0],[1]]). These are the values of the second element in each of the training data entries. After you're done, **save your changes**. (You can use the CMD/CTRL+S keyboard shortcut.)

![Modifying expected outputs](./images/ae6.png?raw=true "Modifying expected outputs")

<br><br>

8. Now, run the neural net again. This time when the weights after training are shown, you should see a bias for a higher weight for the second item.
```
python nn.py
```
![Second run of simple nn](./images/ae7.png?raw=true "Second run of simple nn") 

<br><br>

9. At the input prompts, just input any sequence of 0's and 1's as before.

<br><br>

10. When the trained model then processes your inputs, you should see that it predicts a value that is close to 0 or 1 depending on what your second input was.

![Second output of simple nn](./images/aia-1-9.png?raw=true "Second output of simple nn")

<br><br>

11. (Optional) If you get done early and want more to do, feel free to try other combinations of training inputs and training outputs.
    
<p align="center">
**[END OF LAB]**
</p>
</br></br>

**Lab 2 - Experimenting with Tokenization**

**Purpose: In this lab, we'll see how different models do tokenization.**

1. In the same *llm* directory, we have a simple program that can load a model and print out tokens generated by it. The file name is *tokenizer.py*. You can view the file either by clicking on [**llm/tokenizer.py**](./llm/tokenizer.py) or by entering the command below in the codespace's terminal (assuming you're still in the *genai* directory).

```
code tokenizer.py
```

<br><br>

2. This program can be run and passed a model to use for tokenization. To start, we'll be using a model named *bert-base-uncased*. Let's look at this model on huggingface.co.  Go to https://huggingface.co/models and in the *Models* search area, type in *bert-base-uncased*. Select the entry for *google-bert/bert-base-uncased*.

![Finding bert model on huggingface](./images/aia-1-10.png?raw=true "Finding bert model on huggingface")

<br><br>

3. Once you click on the selection, you'll be on the *model card* tab for the model. Take a look at the model card for the model and then click on the *Files and Versions* and *Community* tabs to look at those pages.

![huggingface tabs](./images/aia-1-11.png?raw=true "huggingface tabs")

<br><br>

4. Now let's switch back to the codespace and, in the terminal, run the *tokenizer* program with the *bert-base-uncased* model. Enter the command below. This will download some of the files you saw on the *Files* tab for the model in HuggingFace.

```
python tokenizer.py bert-base-uncased
```

<br><br>

5. After the program starts, you will be at a prompt to *Enter text*. Enter in some text like the following to see how it will be tokenized.

```
This is sample text for tokenization and text for embeddings.
```

![input for tokenization](./images/ae9.png?raw=true "input for tokenization")

<br><br>

6. After you enter this, you'll see the various subword tokens that were extracted from the text you entered. And you'll also see the ids for the tokens stored in the model that matched the subwords.

![tokenization output](./images/aia-1-13.png?raw=true "tokenization output")

<br><br>

7. Next, you can try out some other models by repeating steps 4 - 6 for other tokenizers like the following. (You can use the same text string or different ones. Notice how the text is broken down depending on the model and also the meta-characters.)
```
python tokenizer.py roberta-base
python tokenizer.py gpt2
python tokenizer.py xlnet-large-cased
```

<br><br>

8. (Optional) If you finish early and want more to do, you can look up the models from step 7 on huggingface.co/models.

<br>  
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 3 - Understanding embeddings, vectors and similarity measures**

**Purpose: In this lab, we'll see how tokens get mapped to vectors and how vectors can be compared.**

1. In the repository, we have a Python program that uses a Tokenizer and Model to create embeddings for three terms that you input. It then computes and displays the cosine similarity between each combination. Open the file to look at it by clicking on [**llm/vectors.py**](./llm/vectors.py) or by using the command below in the terminal.


```
code vectors.py
```
<br><br>

2. Let's run the program. As we did for the tokenizer example, we'll pass in a model to use. We'll also pass in a second argument which is the number of dimensions from the vector for each term to show. Run the program with the command below. You can wait to enter terms until the next step.


```
python vectors.py bert-base-cased 5
```

![vectors program run](./images/ae10.png?raw=true "vectors program run")

<br><br>

3. The command we just ran loads up the bert-base-cased model and tells it to show the first 5 dimensions of each vector for the terms we enter. The program will be prompting you for three terms. Enter each one in turn. You can try two closely related words and one that is not closely related. For example
   - king
   - queen
   - duck

![vectors program inputs](./images/ae11.png?raw=true "vectors program inputs")

<br><br>

4. Once you enter the terms, you'll see the first 5 dimensions for each term. And then you'll see the cosine similarity displayed between each possible pair. This is how similar each pair of words is. The two that are most similar should have a higher cosine similarity "score".

![vectors program outputs](./images/aia-1-16.png?raw=true "vectors program outputs")

<br><br>

5. Each vector in the bert-based models have 768 dimensions. Let's run the program again and tell it to display 768 dimensions for each of the three terms.  Also, you can try another set of terms that are more closely related, like *multiplication*, *division*, *addition*.

```
python vectors.py bert-base-cased 768
```

<br><br>

6. You should see that the cosine similarities for all pair combinations are not as far apart this time.

![vectors program second outputs](./images/aia-1-17.png?raw=true "vectors program second outputs")

<br><br>

7. As part of the output from the program, you'll also see the *token id* for each term. (It is above the print of the dimensions. If you don't want to scroll through all the dimensions, you can just run it again with a small number of dimensions like we did in step 2.) If you're using the same model as you did in lab 2 for tokenization, the ids will be the same. 

![token id](./images/aia-1-18.png?raw=true "token id")

<br><br>


8. You can actually see where these mappings are stored if you look at the model on Hugging Face. For instance, for the *bert-base-cased* model, you can go to https://huggingface.co and search for bert-base-cased. Select the entry for google-bert/bert-base-cased. (Make sure you pick the one with that name.)

![finding model](./images/aia-1-19.png?raw=true "finding model")

<br><br>

9. On the page for the model, click on the *Files and versions* tab. Then find the file *tokenizer.json* and click on it. The file will be too large to display, so click on the *check the raw version* link to see the actual content.

![selecting tokenizer.json](./images/aia-1-20.png?raw=true "selecting tokenizer.json")
![opening file](./images/aia-1-21.png?raw=true "opening file")

<br><br>

9. You can search for the terms you entered previously with a Ctrl-F or Cmd-F and find the mapping between the term and the id. If you look for "##" you'll see mappings for parts of tokens like you may have seen in lab 2.

![finding terms in file](./images/aia-1-22.png?raw=true "finding terms in files")

<br>
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 4 - Working with transformer models**

**Purpose: In this lab, we’ll see how to interact with various models for different standard tasks**

1. In our repository, we have several different Python programs that utilize transformer models for standard types of LLM tasks. These programs have some random facts from different categories stored in them to use for transformer models to act on.

<br><br>

2. One of the programs is a simple translation example. The file name is *translation.py*. Open the file either by clicking on [**llm/translation.py**](./llm/translation.py) or by entering the command below in the codespace's terminal. 

```
code translation.py
```
<br><br>

3. Take a look at the file contents.  Notice that we are pulling in a specific model ending with 'en-fr'. This is a clue that this model is trained for English to French translation. Let's find out more about it. In a browser, go to *https://huggingface.co/models* and search for the model name 'Helsinki-NLP/opus-mt-en-fr' (or you can just go to huggingface.co/Helsinki-NLP/opus-mt-en-fr).

![model search](./images/aia-1-23.png?raw=true "model search")

<br><br>

3. You can look around on the model card for more info about the model. Notice that it has links to an *OPUS readme* and also links to download its original weights, translation test sets, etc.

<br><br>

4. When done looking around, go back to the repository and look at the rest of the *translation.py* file. What we are doing is loading the model, the tokenizer, and then taking a set of random texts and running them through the tokenizer and model to do the translation. Go ahead and execute the code in the terminal via the command below.

```
python translation.py
```

![translation by model](./images/ae12.png?raw=true "translation by model")
 
<br><br>

5. There's also an example program for doing classification. The file name is classification.py. Open the file either by clicking on [**llm/classification.py**](./llm/classification.py) or by entering the command below in the codespace's terminal.


```
code classification.py
```

<br><br>

6. Take a look at the model for this one *joeddav/xlm-roberta-large-xnli* on huggingface.co and read about it. When done, come back to the repo.

<br><br>

7. *classification.py* uses a HuggingFace pipeline to do the main work. Notice it also includes a list of categories as *candidate_labels* that it will use to try and classify the data. Go ahead and run it to see it in action. (This will take awhile to download the model.) After it runs, you will see each topic, followed by the ratings for each category. The scores reflect how well the model thinks the topic fits a category. The highest score reflects which category the model thinks fit best.

```
python classification.py
```

![classification by model](./images/aia-1-25.png?raw=true "classification by model")

<br><br>


8. Finally, we have a program to do sentiment analysis - *sentiment.py*. Go ahead and run this one in the similar way and observe which ones it classified as positive and which as negative and the relative scores.

```
python sentiment.py
```

![sentiment by model](./images/aia-1-26.png?raw=true "sentiment by model")

<br><br>

9. In the LLM directory, we have a program that explains some of the key concepts of how transformers work. You can run it via the command below and move through the different explanations displayed on the screen.

```
python transformer_explorer.py
```

![Running transformer explorer](./images/ae87.png?raw=true "Running transformer explorer")

<br><br>

<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 5 - Fine-tuning Models**

**Purpose: In this lab, we'll see how we can fine-tune a model with a set of data to get better responses in a particular domain.**

1. For this lab, we will build out a starter file into a full fine-tuning example. The starter file is in the ft (for "fine tuning") directory. Change into that directory.

```
cd /workspaces/ae-day2/ft
```

<br><br>

2. We'll be tuning a basic model from Hugging Face called *distilbert-base-uncased*. You can see information about that model [here](https://huggingface.co/distilbert/distilbert-base-uncased).

<br><br>

3. We'll be using that model to do sentiment analysis on Amazon product reviews. Sentiment analysis means determining if a review is positive, negative, or other - as we did with one of the examples in Lab 4. The dataset we'll be using for testing and fine-tuning is a *top-level* one on Hugging Face named "Amazon Review Polarity". We can't see that version on Hugging Face, but there is one based on that that you can look at [here](https://huggingface.co/datasets/mteb/amazon_polarity). You can use the *Dataset Viewer* on that page to see the info in the dataset if interested.

![dataset viewer](./images/aia-1-27.png?raw=true "dataset viewer")

<br><br>

4. To build out the actual fine-tuning demo, we'll use a side-by-side compare and merge approach. That means we'll start up an editor with a completed version of the file on the left and the starter version on the right. To do this, run the command below: (the complete code is in [extra/reviews-ft.txt](./extra/reviews-ft.txt)).


```
code -d ../extra/reviews-ft.txt reviews-ft.py
```

![diff](./images/aia-1-28.png?raw=true "diff")

<br><br>

5. After running this command, you'll see the side-by-side editors. Now we want to merge in the sections that are in the left file into the right file to build out our demo. Before you merge in a section, make sure to glance at the block of code to try and understand what it's doing. Then, when ready, hover over the bar between the two versions and an arrow should display. Click on the arrow to merge that section in.

![review and merge](./images/aia-1-29.png?raw=true "review and merge")

<br><br>


6. Proceed down through the remaining differences, quickly reviewing the code to be merged, and then merging it with the arrow. Once you are done, the files should show as the same without any remaining "blocks" of differences. When there are no differences left and you're done, close the diff view by clicking on the "X" in the tab at the top.

![close after merging](./images/aia-1-30.png?raw=true "close after merging")

<br><br>


7.  Now we can run the demo with the following command:

```
python reviews-ft.py
```

![running](./images/ae13.png?raw=true "running")

<br><br>

8. This will first download the Distilbert model and then run through a subset of reviews to see how well the model does (without any fine-tuning) on determining the sentiment of each review. If this goes by too fast, you can scroll back up to see the reviews and results. You will probably see something in the 40-50% success range.

![no fine tuning run](./images/aia-1-32.png?raw=true "no fine tuning run")

<br><br>

9. Now the model will use some Hugging Face transformer library tools to train the model from the dataset. This part will take probably as much as 5-8 minutes. What you are looking for here is the *loss value* to go down as the fine tuning proceeds. The loss value going down means that the model is getting better at predicting the sentiment as it learns from the dataset examples. 

![fine tuning run](./images/ae14.png?raw=true "fine tuning run")

<br><br>

10. At the end of the fine-tuning, the program will once again run through a test of the model's prediction for sentiments on the reviews. This time, since it has been fine-tuned, it should report success more in the 80-90% range.

![after fine tuning run](./images/aia-1-34.png?raw=true "after fine tuning run")

<br>
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 6 - Working with Vector Databases**

**Purpose: In this lab, we’ll learn about how to use vector databases for storing supporting data and doing similarity searches.**

1. For this lab and the following ones, we'll be using files in the *rag* subdirectory. Change to that directory.

```
cd /workspaces/ae-day2/rag
```

<br><br>

2. We have several data files that we'll be using that are for a ficticious company. The files are located in the *rag/knowledge_base_pdfs* directory. [knowledge base pdfs](./rag/knowledge_base_pdfs) You can browse them via the explorer view. Here's a [direct link](./rag/knowledge_base_pdfs/OmniTech_Returns_Policy_2024.pdf) to an example one if you want to open it and take a look at it.

![PDF data file](./images/aia-1-41.png?raw=true "PDF data file") 

<br><br>

3. In our repository, we have some simple tools built around a popular vector database called Chroma. There are two files which will create a vector db (index) for the *.py files in our repo and another to do the same for the pdfs in our knowledge base. You can look at the files either via the usual "code <filename>" method or clicking on [**tools/index_code.py**](./tools/index_code.py) or [**tools/index_pdfs.py**](./tools/index_pdfs.py).

```
code ../tools/index_code.py
code ../tools/index_pdfs.py
```

<br><br>

4. Let's create a vector database of our local code files (python and bash). Run the program to index those. **This may run for a while before you see things happening.** You'll see the program loading the embedding model that will turn the code chunks into numeric represenations in the vector database and then it will read and index our *.py files. It will create a new local vector database in *./chroma_code_db*.

```
python ../tools/index_code.py
```

![Running code indexer](./images/ae15.png?raw=true "Running code indexer")

<br><br>

5. To help us do easy/simple searches against our vector databases, we have another tool at [**tools/search.py**](./tools/search.py). This tool connects to the ChromaDB vector database we create, and, using cosine similarity metrics, finds the top "hits" (matching chunks) and prints them out. You can open it and look at the code in the usual way if you want. No changes are needed to the code.

```
code ../tools/search.py
```

This tool takes a *--target* argument when you run it with a value of either "code" or "pdfs" to indicate which vector database to search.
You can also pass search queries directly on the command line with the *--query* argument. Or you can just start it and type in the queries, hit return, and get results. 
<br><br>

6. Let's run the search tool against the vector database we built in step 4. You can run it with phrases related to our coding like any of the ones shown below. You can run the commands with separate invocations of the tool as shown here, or just run it and enter them in interactive mode.  Notice the top hits and their respective cosine similarity values. Are they close? Farther apart?

```
  python ../tools/search.py --query "convert text to vectors" --target code
  python ../tools/search.py --query "tokenize sentences" --target code
  python ../tools/search.py --query "convert text to numbers" --target code
```

![Running search](./images/ae16.png?raw=true "Running search")

<br><br>

7.  Let's create a vector database based off of the PDF files. Just run the indexer for the pdf file.

```
python ../tools/index_pdfs.py
```

![Indexing PDFs](./images/ae17.png?raw=true "Indexing PDFs")

<br><br>

8. We can run the same search tool to find the top hits for information about the company policies. Below are some prompts you can try here. Notice the cosine similarity values on each - are they close? Farther apart?  When done, just type "exit".

```
  python ../tools/search.py --query "track my shipment" --target pdfs
  python ../tools/search.py --query "forgot my login credentials" --target pdfs
  python ../tools/search.py --query "exchange damaged item" --target pdfs
```

![PDF search](./images/ae18.png?raw=true "PDF search")

<br><br>

9. Keep in mind that this is not trying to intelligently answer your prompts at this point. This is a simple semantic search to find related chunks. In lab 7, we'll add in the LLM to give us better responses. 

<br>
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>


**Lab 7: Building a Complete RAG System**

**Purpose: In this lab, we'll create a complete RAG (Retrieval-Augmented Generation) system that retrieves relevant context from our vector database and uses an LLM to generate intelligent, grounded answers.**

1. You should still be in the *rag* subdirectory. We're going to build a TRUE RAG system that combines vector search with LLM generation. 

<br><br>

2. First, let's examine our complete RAG implementation. We have a completed version and a skeleton version. Use the diff command to see the differences:

```
code -d ../extra/rag_complete.txt rag_code.py
```

![Diff](./images/aia-1-42.png?raw=true "Diff")

<br><br>

3. Once you have the diff view open, take a moment to look at the structure in the complete version on the left. Notice the three main methods: `retrieve()` for finding chunks, `build_prompt()` for augmenting with context, and `generate()` for calling the LLM. These are the three steps of RAG.

- Lines 95-157: `retrieve()` - semantic search in ChromaDB
- Lines 159-209: `build_prompt()` - combining context with the question
- Lines 211-273: `generate()` - calling Ollama's Llama 3.2 model

<br><br>

4. Now, as you've done before, merge the code segments from the complete file (left side) into the skeleton file (right side) by clicking the arrow pointing right in the middle bar for each difference. Start with the comments section at the top, then work your way down through the class methods.

![Merge](./images/aia-1-43.png?raw=true "Merge")

<br><br>

5. After merging all the changes, double-check that there are no remaining diffs (red blocks on the side). Then close the diff view by clicking the "X" in the tab. 

![Completed](./images/aia-1-44.png?raw=true "Completed")

<br><br>

6. Now let's run our complete RAG system:

```
python rag_code.py
```

The system will connect to the vector database we created in Lab 6 and check if Ollama is running.

<br><br>

7. You should see knowledge base statistics showing how many chunks are indexed, and a check that Ollama is running with the llama3.2:1b model. If you see any errors about Ollama not running, check that with "ollama list".  If Ollama doesn't respond, try "ollama serve &".

![Running](./images/ae19.png?raw=true "Running")

<br><br>

8. Now you'll be at a prompt to ask questions. Try this first question:

```
How can I return a product?
```

Watch what happens - the system will show you the three RAG steps in the logs:
- **[RETRIEVE]** Finding relevant chunks in the vector database
- **[AUGMENT]** Building a prompt with context
- **[GENERATE]** Querying Llama 3.2 to generate an answer

<br><br>

9. After a few seconds, you'll see an ANSWER section with the LLM-generated response, followed by a SOURCES section showing which PDFs and pages were used. Notice how the answer is much more complete and natural than just showing search results.

![Answer](./images/ae20.png?raw=true "Answer")

<br><br>

10. Try a few more questions to see RAG in action:

```
What are the shipping costs?
How do I reset my password?
What should I do if my device won't turn on?
```

For each question, notice how the system retrieves relevant chunks and generates a complete answer based on that context.

![Answer](./images/ae21.png?raw=true "Answer")

<br><br>

11. Now try asking a question that's NOT in the PDFs to see how RAG handles it:

```
What's the CEO's favorite color?
```

![Answer](./images/aia-1-48.png?raw=true "Answer")

Notice how the system should say it doesn't have that information (rather than making something up). This is the "grounding" benefit of RAG - answers are based on actual documents.

<br><br>

12. When you're done experimenting, type `quit` to exit the system.

<br><br>


**Key Takeaways:**
- You've built a TRUE RAG system that combines vector search with LLM generation
- RAG has three steps: Retrieve relevant chunks, Augment the prompt with context, Generate answers with an LLM
- The system uses ChromaDB for semantic search and Ollama/Llama 3.2 for generation
- RAG answers are grounded in your documents - reducing hallucination compared to pure LLM queries
- The system can cite sources, showing which documents and pages were used

<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 8 - Graph RAG**

**Purpose: In this lab, we'll see how Graph RAG works via leveraging frameworks and using LLMs to help generate queries.**

1. To do this lab, first we need a neo4j instance running to manage the graph database. We'll use a docker image for this that is already populated with data for us. Change to the neo4j directory, set an environment variable for the DOCKER version and run the script with the "2" parameter. This will take a few minutes to build and start. Be sure to add the "&" to run this in the background.

(When it is ready, you may see a "*INFO  [neo4j/########] successfully initialized:*" message or one that says "naming to docker.io/library/neo4j:custom".) Just hit *Enter* and you can change back to the *workspaces/ae-day2* subdirectory. 

```
cd /workspaces/ae-day2/neo4j

export DOCKER_API_VERSION=1.43

./neo4j-setup.sh 2 &

cd ..
``` 

<br><br>

2. This graph database is prepopulated with a large set of nodes and relationships related to movies. This includes actors and directors associated with movies, as well as the movie's genre, imdb rating, etc. You can take a look at the graph nodes by running the following commands in the terminal. **You should be in the "root" directory (/workspaces/ae-day2) when you run these commands.**

```
npm i -g http-server
http-server
```

<br><br>

3. Go to a web browser and open [http://localhost:8080/index.html](http://localhost:8080/index.html) After a moment, you should see a pop-up dialog that you can click on to open a browser to see some of the nodes in the graph. It will take a minute or two to load and then you can zoom in by using your mouse (roll wheel) to see more details.

![loading nodes](./images/ae22.png?raw=true "loading nodes")
![graph nodes](./images/ae23.png?raw=true "graph nodes")


<br><br>

4. When done, you can stop the *http-server* process with *Ctrl-C*. Now, let's go back and create a file to use the langchain pieces and the llm to query our graph database. Change back to the *genai* directory and create a new file named lab8.py.
```
cd ../rag
code lab8.py
```

<br><br>

5. First, add the imports from *langchain* that we need. Put the following lines in the file you just created.
```
import subprocess

from langchain_neo4j import Neo4jGraph, GraphCypherQAChain
from langchain_ollama import OllamaLLM
```

<br><br>

6. Now, let's add the connection to the graph database, including dynamically getting the container IP. Add the following to the file.
```
# Dynamically get Neo4j container IP
def get_neo4j_url():
    try:
        result = subprocess.run(
            ["docker", "inspect", "-f", "{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}", "neo4j"],
            capture_output=True,
            text=True,
            check=True
        )
        container_ip = result.stdout.strip()
        if container_ip:
            return f"bolt://{container_ip}:7687"
    except subprocess.CalledProcessError:
        pass

    # Fallback to localhost
    return "bolt://localhost:7687"

graph = Neo4jGraph(
    url=get_neo4j_url(),
    username="neo4j",
    password="neo4jtest",
    enhanced_schema=False,
)
```

<br><br>


7. Next, let's create the chain instance that will allow us to leverage the LLM to help create the Cypher query and help frame the answer so it makes sense. We'll use Ollama and our llama3 model for both the LLM to create the Cypher queries and the LLM to help frame the answers.
```
chain = GraphCypherQAChain.from_llm(
    cypher_llm=OllamaLLM(model="llama3.2:latest", temperature=0),
    qa_llm=OllamaLLM(model="llama3.2:latest", temperature=0),
    graph=graph,
    verbose=True,
    allow_dangerous_requests=True,
)
```

<br><br>

8. Finally, let's add the code loop to take in a query and invoke the chain. After you've added this code, save the file.
```
while True:
    query = input("\nQuery: ")
    if query == "exit":
        break
    if query.strip() == "":
        continue
    response = chain.invoke({"query": query})
    print(response["result"])
```

<br><br>

9. Now, run the code.
```
python lab8.py
```

<br><br>

10. You can prompt it with queries related to the info in the graph database, like below. (Notes: Actual output length may be limited by the framework. Because of the small model we are using, other queries may or may not work well.)
```
Which movies are comedies?
```

![querying the graph](./images/ae156.png?raw=true "querying the graph")


<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 9 - Hybrid RAG: Semantic Search + Knowledge Graph**

**Purpose: In this lab, we'll build a hybrid RAG system that combines semantic search (ChromaDB) with knowledge graph traversal (Neo4j) to get both precision and context in our answers.**

1. RAG works because semantic search understands meaning. But for precise facts (timeframes, contacts, relationships), a knowledge graph provides structured answers. Combining both gives us the best of both worlds.

For this lab, a knowledge graph has been pre-built from the OmniTech documents. It contains:
- **Entities**: Products, Policies, TimeFrames, Contacts, Conditions, Fees, ShippingMethods, Documents
- **Relationships**: APPLIES_TO, HAS_TIMEFRAME, HANDLES, REQUIRES_CONDITION, HAS_FEE, USES_SHIPPING, CONTAINS

You can view it [here](./neo4j/data3/omnitech_policies.csv) if interested.

<br><br>
  
2. First, let's create the Neo4j graph database with the OmniTech knowledge graph. Run the commands (similar to lab 8) below.

```
cd /workspaces/ae-day2/neo4j
./neo4j-setup.sh 3 &
```

Wait for the message indicating Neo4j is ready. The script will:
- Build a Docker image with the OmniTech schema
- Start Neo4j container on ports 7474 (web) and 7687 (Bolt)
- Auto-initialize the knowledge graph via APOC

When done, you will see a message ending with "Then run:    MATCH (n) RETURN count(n);". This is informational and you can just hit *Enter/Return* to get back to the prompt.

![building graph db](./images/ae25.png?raw=true "building graph db")

<br><br>

3. Change back to the *rag* directory. Then we'll build out the hybrid RAG system as *lab9.py* with the diff and merge process that we've used before. The second command below will start up the editor session.
   
```
cd /workspaces/ae-day2/rag
code -d ../extra/lab9-changes.txt lab9.py
```

![building hybrid code](./images/ae26.png?raw=true "building hybrid code")

<br><br>

4. What you'll see here is that most of the merges are comment sections explaining what the code does (plus some for the prompt, etc.). You can review and merge them as we've done before. After looking over the change, hover over the middle section and click the arrow to merge. Continue with this process until there are no more differences. Then click on the "X" in the tab at the top to close and save your changes.

<br><br>
 

5. Now let's run the hybrid RAG demo with the command below (in the *code* directory). This will then be waiting for you to type in a query.

```
python lab9.py
```

![running](./images/ae27.png?raw=true "running")

<br><br>

6. Let's try a basic query for the return policy. Type in the query below and hit *Enter/Return*.

```
What is the return window for Pro-Series equipment and who do I contact?
```

![running query](./images/ae29.png?raw=true "running query")

7. Watch the output - the demo asks the same question using three different methods: (Ignore any "onnxruntime" warnings.)

You'll see:
- **METHOD 1: SEMANTIC** - Finds document chunks with similar meaning
- **METHOD 2: GRAPH** - Traverses Neo4j relationships via Cypher
- **METHOD 3: HYBRID** - Combines both for precision + context

Each method shows:
- What it retrieved (chunks vs graph nodes)
- The LLM-generated answer based on that context

<br><br>

You can compare the results:

| Method | What it found | Strength |
|--------|---------------|----------|
| SEMANTIC | Document chunks mentioning Pro-Series | Good context, handles vocabulary mismatch |
| GRAPH | Pro_Series → Pro_Series_Return → 14_Days | Precise facts via Cypher traversal |
| HYBRID | Graph facts + Document context | Combines both worlds |

![multiple answers](./images/ae28.png?raw=true "multiple answers")

<br><br>

8. Let's try another query that may benefit more from having the graph db involved. Enter the one below.
   
```
Who handles defective items?
```

<br><br>


9. Notice again the variations in the responses. Typically, because of the direct mapping, the *HYBRID* and *GRAPH* responses will have the best information. Also, the *HYBRID* option may outline exceptions.

![2nd query](./images/ae30.png?raw=true "2nd query")

<br><br>
    
Notice the `HybridRAG` class connects to both databases:
- **ChromaDB** for semantic search (lines 46-53)
- **Neo4j** for graph search (lines 55-67)

<br><br>

10. Discussion Points:
- **Semantic search** (ChromaDB) understands MEANING - handles "money back" → "refund"
- **Graph search** (Neo4j) understands STRUCTURE - traverses entity relationships
- **Cypher queries** navigate: `Product → Policy → TimeFrame → Contact`
- **Hybrid** combines both: graph precision + semantic context
- This mirrors production RAG architectures used by enterprises

<br><br>

11. [OPTIONAL] You can also visualize the knowledge graph. Start a local web server:

```
cd /workspaces/ae-day2/neo4j/data3/public
npx http-server
```

Click the pop-up to open the browser and see the graph visualization. You can move around using the mouse and also zoom in and out. 

![OmniTech Knowledge Graph](./images/ae31.png?raw=true "OmniTech Knowledge Graph")

<br><br>

**NOTE**: If you can't open the page, you may need to go back to the codespace, go to the *PORTS* tab (next to *TERMINAL*), right-click, and set the *Visibility* field to *Public*. See screenshot below.

![make port public](./images/ragv2-26.png?raw=true "make port public")

After that, refresh the page and try again. You may still have to click through another page to allow access.

<br><br>

**Key Takeaway:**
> Semantic search understands MEANING. Graph search understands STRUCTURE. Together they provide comprehensive, accurate answers.

<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>


**Lab 10 - RAG Evaluation and Quality Metrics**

**Purpose: In this lab, we'll learn how to evaluate RAG system quality - a critical concern for enterprise deployments where accuracy, reliability, and answer traceability are paramount.**

1. You should still be in the *rag* subdirectory. We're going to build a RAG evaluation system that measures retrieval quality, answer accuracy, and detects potential hallucinations. First, let's examine our evaluation implementation. We have a completed version and a skeleton version. Use the diff command to see the differences:

```
code -d ../extra/lab10_eval_complete.txt lab10.py
```

![diff](./images/ae88.png?raw=true "diff")

<br><br>

2. Once you have the diff view open, take a moment to look at the structure in the complete version on the left. Notice the key evaluation metrics:
   - **Context Relevance**: How relevant are retrieved chunks to the question?
   - **Answer Groundedness**: Is the answer supported by the context?
   - **Answer Completeness**: Does the answer address all parts of the question?
   - **Hallucination Detection**: Is the LLM making unsupported claims?

<br><br>

3. Now, merge the code segments from the complete file (left side) into the skeleton file (right side) by clicking the arrow pointing right in the middle bar for each difference. Start with the docstrings and comments at the top, then work your way down through the evaluation methods.

<br><br>

4. After merging all the changes, double-check that there are no remaining diffs (red blocks on the side). Then close the diff view by clicking the "X" in the tab.

<br><br>

5. Now let's run our RAG evaluation system:

```
python lab10.py
```

The system will connect to the vector database we created earlier and present you with options.

![options](./images/ae89.png?raw=true "options")

<br><br>

6. You should see a menu with options to evaluate a single question or run a full test suite. Select option **1** to evaluate a single question. Enter a question like:

```
How do I reset my password?
```

![question](./images/ae90.png?raw=true "question")

<br><br>

7. Watch the evaluation process - the system will:
   - **[1/5]** Retrieve relevant context chunks
   - **[2/5]** Generate an answer using the LLM
   - **[3/5]** Evaluate context relevance (LLM-as-judge)
   - **[4/5]** Check answer groundedness (is it supported by context?)
   - **[5/5]** Assess answer completeness

<br><br>

8. After evaluation, you'll see color-coded scores:
   - **GREEN (0.8+)**: Excellent quality
   - **YELLOW (0.6-0.8)**: Acceptable, room for improvement
   - **RED (below 0.6)**: Needs attention

Notice the **OVERALL SCORE** which weights the metrics based on enterprise priorities (groundedness is most important at 40%).

![run](./images/arag24.png?raw=true "run")

<br><br>

9. Here's a few more questions to see how scores vary: (Due to time constraints, it's suggested to pick one.)

```
What is the return policy for products?
What are the shipping costs?
Who is the CEO of OmniTech?
```

Notice how the last question (about the CEO) should show lower groundedness if that information isn't in the documents.

![run](./images/arag26.png?raw=true "run")

<br><br>

**Steps 10-12 are optional and may take longer than lab time allows.**

<br>

10. Now select option **2** to run the full test suite. This runs evaluation on a predefined set of questions with expected keywords - simulating automated regression testing.

<br><br>

11. After the test suite completes, you'll see aggregate metrics for your entire RAG system. This is how enterprises monitor RAG quality:
   - Track metrics over time
   - Set quality thresholds for production readiness
   - Compare different RAG configurations


![system test](./images/arag27.png?raw=true "system test")

<br><br>

12. Discussion Points:
   - **Why is groundedness critical?** Hallucinated answers can cause real business damage
   - **LLM-as-judge approach**: Using one LLM to evaluate another LLM's output
   - **Automated testing**: Test suites enable continuous quality monitoring
   - **Enterprise compliance**: Evaluation metrics provide audit trails for regulated industries

<br><br>

<br>
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 11 - Query Transformation and Re-ranking**

**Purpose: In this lab, we'll implement advanced retrieval techniques that dramatically improve RAG quality - query transformation (expansion, multi-query, HyDE) and two-stage retrieval with re-ranking.**

1. You should still be in the *code* subdirectory. We're going to build an advanced RAG system that transforms user queries for better retrieval and re-ranks results for higher precision. Use the diff command to examine the implementation:

```
code -d ../extra/lab11_rerank_complete.txt lab11.py
```

![diff and merge](./images/ae91.png?raw=true "diff and merge")

<br><br>

2. Once you have the diff view open, look at the key techniques in the complete version:
   - **Query Expansion**: Add synonyms and related terms
   - **Multi-Query**: Generate multiple query variations
   - **HyDE**: Generate hypothetical answers to search for
   - **Re-ranking**: Score and reorder retrieved chunks

<br><br>

3. Merge all the code segments from the complete file into the skeleton file, starting from the top. Pay attention to the prompt templates used for each transformation technique.

<br><br>

4. After merging, close the diff view. Now let's run the advanced RAG demo:

```
python lab11.py
```

![run](./images/ae92.png?raw=true "run")

<br><br>

5. You'll see a menu explaining the different retrieval methods. Each has a color code:
   - **RED (BASIC)**: Standard vector search (baseline)
   - **YELLOW (EXPANSION)**: Query expanded with synonyms
   - **GREEN (MULTI-Q)**: Multiple query variations
   - **CYAN (HYDE)**: Hypothetical document embedding
   - **MAGENTA (RERANK)**: Two-stage with re-ranking

<br><br>

6. Select option **1** to compare all methods on a query. Enter a short, ambiguous query:

```
money back
```

![run](./images/ae93.png?raw=true "run")

This is intentionally vague - notice how the different methods handle vocabulary mismatch (documents might say "refund" or "return" instead of "money back").

<br><br>

7. Watch the output as each method processes the query:
   - **Query Expansion** adds synonyms like "refund", "reimbursement"
   - **Multi-Query** generates variations like "refund process", "return policy"
   - **HyDE** generates an ideal answer and searches for similar content
   - **Re-ranking** retrieves more candidates then scores them precisely

![process](./images/ae94.png?raw=true "process")

<br><br>

8. Compare the answers from each method. Notice how:
   - Basic search might miss relevant documents
   - Expanded queries find more related content
   - HyDE often finds the most relevant passages
   - Re-ranking improves precision (relevant docs ranked higher)

![response](./images/arag30.png?raw=true "response")

<br><br>

9. Now select option **2** to try individual techniques. Choose **3 (HyDE)** and enter:

```
how long to return
```

![response](./images/arag32.png?raw=true "response")

Notice how HyDE generates a hypothetical answer like "Customers may return products within 30 days..." and uses THAT to search - bridging the gap between question-style and answer-style text.

<br><br>

10. Try the **Re-ranking** technique (option 4) with:

```
shipping options and costs
```

![response](./images/arag31.png?raw=true "response")

Notice how re-ranking retrieves 6 candidates (2x the final count) and then scores each one's relevance to return only the top 3 most relevant.

<br><br>

11. Discussion Points:
   - **Query-document mismatch**: Users ask questions, but documents contain answers
   - **HyDE insight**: Searching with answer-like text finds answer-containing documents
   - **Re-ranking trade-off**: More compute for higher precision
   - **Combining techniques**: Production systems often use multiple approaches
   - **Enterprise value**: Better retrieval = better answers from same knowledge base

<p align="center">
**[END OF LAB]**
</p>
</br></br>

**Lab 12 - Corrective RAG (CRAG)**

**Purpose: In this lab, we'll implement Corrective RAG (CRAG), an advanced technique where the system evaluates its own retrieval quality and takes corrective action when results are insufficient - including falling back to web search.**

1. You should still be in the *code* subdirectory. We're going to build a self-correcting RAG system that "knows when it doesn't know" and takes corrective action. Use the diff command to examine the implementation:

```
code -d ../extra/lab12_crag_complete.txt lab12.py
```

<br><br>

2. Once you have the diff view open, look at the CRAG workflow in the complete version:
   - **Retrieval Grader**: Evaluates relevance of each retrieved document
   - **Decision Logic**: CORRECT / AMBIGUOUS / INCORRECT based on scores
   - **Corrective Actions**: Web search fallback, document filtering
   - **Knowledge Refinement**: Extract only relevant information
   - **Answer Generation**: Generate with confidence-appropriate prompts

![response](./images/ae95.png?raw=true "response")

<br><br>

3. Merge all the code segments from the complete file into the skeleton file. Pay special attention to the evaluation prompts and decision thresholds.

<br><br>

4. After merging, close the diff view. Now let's run the CRAG demo:

```
python lab12.py
```

<br><br>

5. You'll see a menu with options and the CRAG decision legend:
   - **GREEN (CORRECT)**: High relevance - use retrieved documents
   - **YELLOW (AMBIGUOUS)**: Partial relevance - refine + supplement with web
   - **RED (INCORRECT)**: Low relevance - fall back to web search

![run](./images/arag34.png?raw=true "run")

<br><br>

6. Select option **1** to run a CRAG query. First, try a question that SHOULD be in the knowledge base:

```
What is the return policy for products?
```

Watch the 6-step CRAG pipeline execute:
- Retrieves 5 documents (more than typical RAG)
- Evaluates each document's relevance (0.0-1.0)
- Makes a decision (likely CORRECT for this query)
- Uses filtered documents without web search
- Refines knowledge and generates answer

<br><br>

7. Notice the visual relevance bars showing each document's score. Documents above 0.7 are considered highly relevant (green), 0.4-0.7 are ambiguous (yellow), and below 0.4 are irrelevant (red).

![known](./images/arag35.png?raw=true "known")

<br><br>

8. Now try a question that's likely NOT in the knowledge base:

```
What is the current stock price of OmniTech?
```

Watch the system detect low relevance and trigger web search (simulated). This is CRAG in action - it "knows when it doesn't know."

![known](./images/arag36.png?raw=true "known")

<br><br>

9. Now select option **2** to compare CRAG vs Standard RAG on a question. Enter:

```
How do I contact support for warranty issues?
```

![known](./images/arag37.png?raw=true "known")

Compare the answers - CRAG should provide a more complete response by intelligently filtering or supplementing the context.

![known](./images/arag38.png?raw=true "known")

<br><br>

10. Try the comparison with a question outside the knowledge base:

```
What are the latest AI developments in 2026?
```

![unknown](./images/arag39.png?raw=true "unknown")

Notice how Standard RAG might hallucinate or give a vague answer, while CRAG recognizes the retrieval failure and seeks external information.

<br><br>

12. Discussion Points:
   - **Self-awareness**: CRAG "knows when it doesn't know" - critical for enterprise trust
   - **Graceful degradation**: Falls back to external search rather than hallucinating
   - **Relevance thresholds**: Tunable parameters (0.7/0.4) for your quality requirements
   - **Audit trail**: CRAGResult tracks every decision for compliance/debugging
   - **Production patterns**: Real systems use actual web search APIs (Google, Bing, Tavily)
   - **Cost trade-off**: More LLM calls for evaluation, but better answer quality

<br>
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Key Takeaway:**
> Semantic search understands MEANING. Graph search understands STRUCTURE. Together they provide comprehensive, accurate answers.

<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>


<p align="center">
<b>For educational use only by the attendees of our workshops.</b>
</p>

<p align="center">
<b>(c) 2026 Tech Skills Transformations and Brent C. Laster. All rights reserved.</b>
</p>
